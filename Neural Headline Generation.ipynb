{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset from Hugging Face (if available)\n","dataset = load_dataset(\"gigaword\",trust_remote_code=True)\n","\n","# Split into training, validation, and test sets\n","train_data = dataset[\"train\"]\n","val_data = dataset[\"validation\"]\n","test_data = dataset[\"test\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n","\n","def preprocess_data(example):\n","    inputs = tokenizer(example[\"document\"], max_length=100, truncation=True, padding=\"max_length\")\n","    targets = tokenizer(example[\"summary\"], max_length=50, truncation=True, padding=\"max_length\")\n","\n","    return {\n","        \"input_ids\": inputs[\"input_ids\"],\n","        \"attention_mask\": inputs[\"attention_mask\"],\n","        \"labels\": targets[\"input_ids\"]\n","    }\n","\n","# Apply tokenization\n","train_data = train_data.map(preprocess_data, batched=True)\n","val_data = val_data.map(preprocess_data, batched=True)\n","test_data = test_data.map(preprocess_data, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader\n","import os\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","\n","\n","train_data_sample = train_data.select(range(256000))  # Use only 100 samples\n","val_data_sample = val_data.select(range(20))  # Use 20 samples\n","test_data_sample = test_data.select(range(256))  # Use 20 samples\n","\n","train_data_sample.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","val_data_sample.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","test_data_sample.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n","train_loader = DataLoader(train_data_sample, batch_size=256, shuffle=True)\n","val_loader = DataLoader(val_data_sample, batch_size=256, shuffle=False)\n","test_loader = DataLoader(test_data_sample, batch_size=256, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","class VariationalEncoder(nn.Module):\n","    def __init__(self, input_size=50265, hidden_size=250, embedding_dim = 300, n_layers=1, bi_dir=True ):\n","        super(VariationalEncoder, self).__init__()\n","        self.bi = 2 if bi_dir else 1\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        self.embedding = nn.Embedding(input_size, embedding_dim)\n","        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers, batch_first=True, bidirectional=bi_dir)\n","\n","\n","    def forward(self, input, intial_hidden , attention_mask):\n","        seq_len = len(input)\n","        embedded = self.embedding(input)\n","        if attention_mask is not None:\n","            embedded = embedded * attention_mask.unsqueeze(-1)\n","        output, hidden = self.gru(embedded, intial_hidden)\n","        return output\n","\n","    def initHidden(self , batch_size):\n","        return torch.zeros( self.n_layers * self.bi, batch_size , self.hidden_size, device=device)\n","\n","class latent(nn.Module):\n","    def __init__(self, input_size=50265, hidden_size=500, embedding_dim = 300, latent_size=500, n_layers=1, bi_dir=True):\n","        super(latent, self).__init__()\n","        self.wyh = nn.Linear(embedding_dim, hidden_size)\n","        self.wzh = nn.Linear(latent_size, hidden_size)\n","        self.whh = nn.Linear(hidden_size, hidden_size)\n","        self.bh = nn.Parameter(torch.randn(hidden_size))\n","        self.sigmoid = nn.Sigmoid()\n","        self.fc_mu = nn.Linear(hidden_size , latent_size)\n","        self.fc_logvar = nn.Linear(hidden_size, latent_size)\n","    def forward(self, output_embedding,decoder_hidden,previous_z):\n","        h_z = self.sigmoid(self.wyh(output_embedding) + self.wzh(previous_z) + self.whh(decoder_hidden) + self.bh)\n","        mu = self.fc_mu(h_z)\n","        logvar = self.fc_logvar(h_z)\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        z_t = mu + (eps * std)\n","        return  h_z , z_t\n","\n","class generate(nn.Module):\n","    def __init__(self, input_size=50265, hidden_size=500, embedding_dim = 300, latent_size=500, n_layers=1, bi_dir=True):\n","        super(generate, self).__init__()\n","        self.wzh = nn.Linear(latent_size, hidden_size)\n","        self.whh = nn.Linear(hidden_size, hidden_size)\n","        self.bh = nn.Parameter(torch.randn(hidden_size))\n","        self.softmax = nn.LogSoftmax()\n","        self.why = nn.Linear(hidden_size, input_size)\n","        self.bhy = nn.Parameter(torch.randn(input_size))\n","    def forward(self , z_t , hidden_secondgru,):\n","        h_y = torch.tanh(self.wzh(z_t)+ self.whh(hidden_secondgru))\n","        y_t = self.why(h_y) + self.bhy\n","        return y_t\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, input_size=50265, hidden_size=500, embedding_dim = 300, latent_size=500, n_layers=1, bi_dir=False  ):\n","        super(Attention, self).__init__()\n","        self.wd = nn.Linear(hidden_size, hidden_size)\n","        self.we = nn.Linear(hidden_size, hidden_size)\n","        self.v = nn.Parameter(torch.randn(hidden_size))\n","        self.ba = nn.Parameter(torch.randn(hidden_size))\n","    def score(self, dec_st, enc_st):\n","        alpha = torch.matmul(torch.tanh(self.wd(dec_st.permute(1,0,2)) + self.we(enc_st) + self.ba), self.v.unsqueeze(1))\n","        return alpha\n","\n","    def forward(self, dec_state, enc_states):\n","        alpha = self.score(dec_state, enc_states)\n","        alpha = torch.softmax(alpha, dim=1)\n","        alpha = alpha.permute(0, 2, 1)\n","        return torch.bmm(alpha, enc_states), alpha\n","\n","\n","\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size= 500, output_size = 50265, attn_type = 'gen', n_layers=1, dropout_p=0.1, bi_dir=False , input_dim = 50265 , embedding_dim = 300):\n","        super(AttnDecoderRNN, self).__init__()\n","        if bi_dir == True:\n","            self.bi = 2\n","        else:\n","            self.bi = 1\n","        self.hidden_size = hidden_size\n","        self.maxlen = 0\n","        self.attn_type = attn_type\n","\n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers,  dropout=dropout_p,\n","                          batch_first=True, bidirectional = bi_dir)\n","        self.gru2 = nn.GRU(embedding_dim + hidden_size, hidden_size, 1, dropout=dropout_p, batch_first=True, bidirectional=False)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.out2 = nn.Linear(hidden_size*2, output_size)\n","        self.latent = latent().to(device)\n","        self.generate = generate().to(device)\n","        self.attention = Attention().to(device)\n","\n","    def forward(self, input, previous_hidden1 , previous_hidden2 , encoder_hidden , hidden_latent_previous ):\n","        output_embedding = self.embedding(input)\n","        output_gru1, hidden1 = self.gru(output_embedding, previous_hidden1.permute(1, 0, 2))\n","        dec_out = hidden1\n","        c_t, attn_wt = self.attention(dec_out, encoder_hidden)\n","        gru2_input = torch.cat((output_embedding, previous_hidden2.permute(1, 0, 2)), dim=-1)  # (batch_size, 1, hidden_size * 2)\n","        output_gru2, hidden2 = self.gru2(gru2_input, c_t.permute(1,0,2))\n","        hidden_latent , z_t = self.latent(output_embedding , previous_hidden2.permute(1,0,2) , hidden_latent_previous)\n","        final_output = self.generate(z_t , output_gru2)\n","        return final_output, hidden1.permute(1,0,2), hidden2.permute(1,0,2), attn_wt , hidden_latent\n","\n","class Seq2Seq_Model(nn.Module):\n","        def __init__(self):\n","            super().__init__()\n","            self.encoder = VariationalEncoder().to(device)\n","            self.decoder = AttnDecoderRNN().to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","from torch.autograd import Variable\n","from tqdm import tqdm\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ---------------------------\n","#  Model Parameters\n","# ---------------------------\n","hidden_size = 500\n","vocab_size = 50265  # Matches input_size/output_size in your model\n","n_layers = 1\n","learning_rate = 0.5\n","teacher_forcing_ratio = 0  # 50% Teacher Forcing\n","clip = 5.0\n","num_epochs = 10 # Change as needed\n","\n","# ---------------------------\n","#  Initialize Model, Loss, Optimizer\n","# ---------------------------\n","\n","seq_Model = Seq2Seq_Model().to(device)\n","\n","seq_Model_optimizer = optim.Adadelta(seq_Model.parameters(), lr=learning_rate)\n","criterion = nn.NLLLoss(ignore_index=1)  # Ignore PAD token (assumed to be 0)\n","\n","def train_epoch(seq_Model, dataloader, criterion, seq_Model_optimizer, device):\n","    seq_Model.train()\n","    total_loss = 0.0\n","    teacher_forcing_ratio_2 = 0.0\n","    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\")\n","    for i , batch in progress_bar:\n","        # Reset gradients\n","        seq_Model.zero_grad()\n","        # Load data and move to GPU\n","        input_tensor = batch[\"input_ids\"].to(device)  # Source sentence (batch, src_len)\n","        target_tensor = batch[\"labels\"].to(device)  # Target sentence (batch, tgt_len)\n","        attention_mask = batch[\"attention_mask\"].to(device)  # Attention mask\n","        batch_size = input_tensor.size(0)\n","        # Initialize encoder hidden state\n","        batch_size = batch[\"input_ids\"].size(0)\n","        encoder_hidden = seq_Model.encoder.initHidden(batch_size)\n","        encoder_hidden = seq_Model.encoder(input_tensor, encoder_hidden , attention_mask)\n","        # Decoder: Start with SOS token (assuming SOS = 1)\n","        decoder_input = torch.tensor([[1]] * batch_size, device=device)  # (batch, 1)\n","        FristGRU_decoder_hidden = encoder_hidden.mean(dim=1 , keepdim = True)\n","        SecondGRU_decoder_hidden = encoder_hidden.mean(dim=1 , keepdim=True)\n","        hidden_latent_previous = torch.zeros(256 , 1 , 500).to(device)\n","        loss = 0\n","        use_teacher_forcing = random.random() < teacher_forcing_ratio_2\n","        c_t_prev=None\n","        if use_teacher_forcing:\n","            for t in range(target_tensor.size(1)-1):\n","                decoder_output, FristGRU_decoder_hidden,SecondGRU_decoder_hidden, _ , hidden_latent_previous = seq_Model.decoder(decoder_input, FristGRU_decoder_hidden,SecondGRU_decoder_hidden.permute(1,0,2) , encoder_hidden , hidden_latent_previous)\n","                decoder_output = decoder_output.squeeze(1)\n","                decoder_output = F.log_softmax(decoder_output , dim = -1)\n","                if(torch.isnan(criterion(decoder_output, target_tensor[:, t+1])) == True):\n","                    break\n","                loss += criterion(decoder_output, target_tensor[:, t+1] )  # Compute loss\n","                decoder_input = target_tensor[:, t+1]  # Next input is the correct word\n","                decoder_input = decoder_input.unsqueeze(1)\n","        else:\n","            for t in range(target_tensor.size(1) - 1):\n","                decoder_output, FristGRU_decoder_hidden,SecondGRU_decoder_hidden, _ , hidden_latent_previous = seq_Model.decoder(decoder_input, FristGRU_decoder_hidden,SecondGRU_decoder_hidden.permute(1,0,2) , encoder_hidden , hidden_latent_previous)\n","                decoder_output = decoder_output.squeeze(1)\n","                decoder_output = F.log_softmax(decoder_output, dim = -1)\n","                if(torch.isnan(criterion(decoder_output, target_tensor[:, t+1])) == True):\n","                    continue\n","                loss += criterion(decoder_output, target_tensor[:, t+1])\n","                topv, topi = decoder_output.data.topk(1)# Select highest-probability word\n","                decoder_input = topi\n","\n","\n","        # teacher_forcing_ratio = teacher_forcing_ratio * 0.95\n","        teacher_forcing_ratio_2 = 0.95 * teacher_forcing_ratio_2\n","        loss.backward()\n","        seq_Model_optimizer.step()\n","        total_loss += loss.item()\n","        progress_bar.set_postfix(loss=(loss.item() / target_tensor.size(0)))\n","\n","    return total_loss / len(train_loader) \n","\n","for epoch in range(num_epochs):\n","    train_loss = train_epoch(seq_Model, train_loader, criterion, seq_Model_optimizer, device)\n","    print(f\"Epoch {epoch+1}/{num_epochs}  - Train Loss: {train_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","from collections import defaultdict\n","\n","def beam_search_decode(seq_Model, input_tensor, beam_width, max_length,attention_mask, device,length_penalty=0.6, min_length=10):\n","    \n","    seq_Model.eval()\n","    with torch.no_grad():\n","        # Encode the input sequence\n","        batch_size = 1  # Beam search works on one sequence at a time\n","        encoder_hidden = seq_Model.encoder.initHidden(batch_size)\n","        encoder_hidden = seq_Model.encoder(input_tensor.unsqueeze(0), encoder_hidden, attention_mask.unsqueeze(0))\n","\n","        # Initialize the beam with the start token (e.g., <SOS> = 2)\n","        start_token = 1\n","        decoder_input = torch.tensor([[start_token]], device=device)  # (1, 1)\n","        FristGRU_decoder_hidden = encoder_hidden.mean(dim=1, keepdim=True)\n","        SecondGRU_decoder_hidden = encoder_hidden.mean(dim=1, keepdim=True)\n","        hidden_latent_previous = torch.zeros(1, 1, 500).to(device)\n","\n","        # Initialize the beam\n","        beam = [{\n","            'sequence': [1],\n","            'log_prob': 0.0,\n","            'hidden1': FristGRU_decoder_hidden,\n","            'hidden2': SecondGRU_decoder_hidden,\n","            'hidden_latent': hidden_latent_previous\n","        }]\n","\n","        for step in range(max_length):\n","            candidates = []\n","            for candidate in beam:\n","                # Stop expanding if the last token is <EOS> (e.g., <EOS> = 2)\n","                if candidate['sequence'][-1] == 2:\n","                    candidates.append(candidate)\n","                    continue\n","                # Prepare decoder input\n","                decoder_input = torch.tensor([[candidate['sequence'][-1]]]).to(device)  # (1, 1)\n","                # Decode the next token\n","                decoder_output, hidden1, hidden2, _, hidden_latent = seq_Model.decoder(\n","                    decoder_input, candidate['hidden1'], candidate['hidden2'],\n","                     encoder_hidden, candidate['hidden_latent']\n","                )\n","                decoder_output = decoder_output.squeeze(1)\n","                decoder_output = F.log_softmax(decoder_output, dim=-1)  # (1, vocab_size)\n","\n","                # Get the top k tokens and their log probabilities\n","                topk_log_probs, topk_tokens = decoder_output.topk(beam_width, dim=-1)\n","                topk_log_probs = topk_log_probs.squeeze(0).cpu().numpy()  # (beam_width,)\n","                topk_tokens = topk_tokens.squeeze(0).cpu().numpy()  # (beam_width,)\n","                \n","                for i in range(beam_width):\n","                    new_sequence = candidate['sequence'] + [topk_tokens[i]]\n","                    new_log_prob = (float(candidate['log_prob']) + float(topk_log_probs[i]))  # Ensure scalar\n","                    candidates.append({\n","                            'sequence': new_sequence,\n","                            'log_prob': new_log_prob,\n","                            'hidden1': hidden1,\n","                            'hidden2': hidden2,\n","                            'hidden_latent': hidden_latent\n","                        })\n","\n","            candidates = sorted(candidates, key=lambda x: x['log_prob'], reverse=True)[:beam_width]\n","            beam = candidates\n","\n","        print(beam[0]['sequence'])\n","        best_candidate = beam[0]\n","        return best_candidate['sequence'], best_candidate['log_prob']\n","\n","\n","def evaluate_with_beam_search(seq_Model, dataloader, beam_width, max_length, device):\n","    predictions = []\n","    seq_Model.eval()\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Evaluating with Beam Search\"):\n","            input_tensor = batch[\"input_ids\"].to(device)  # (batch_size, src_len)\n","            attention_mask = batch[\"attention_mask\"].to(device)  # Attention mask\n","            batch_size = input_tensor.size(0)\n","            for i in range(20):\n","              sequence, _ = beam_search_decode(\n","                    seq_Model, input_tensor[i], beam_width, max_length,attention_mask[i], device\n","                )\n","              predictions.append(sequence)\n","              \n","\n","    return predictions\n","\n","\n","# Example usage\n","beam_width = 10  # Number of beams\n","max_length = 100  # Maximum sequence length\n","test_predictions = evaluate_with_beam_search(seq_Model, test_loader, beam_width, max_length, device)\n","wrapped_predictions = [test_predictions]\n","print(wrapped_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["decoded_sentences = [\n","    tokenizer.decode(tokens, skip_special_tokens=True).strip()\n","    for batch in wrapped_predictions for tokens in batch\n","]\n","\n","decoded_inputs = [\n","    tokenizer.decode(tokens, skip_special_tokens=True).strip()\n","    for tokens in test_loader.dataset[\"input_ids\"]\n","]\n","\n","decoded_targets = [\n","    tokenizer.decode(tokens, skip_special_tokens=True).strip()\n","    for tokens in test_loader.dataset[\"labels\"]\n","]\n","\n","# Print a few examples\n","for i in range(min(20, len(decoded_sentences))):\n","    print(f\"Example {i+1}:\")\n","    print(f\"Input: {decoded_inputs[i]}\")\n","    print(f\"Target: {decoded_targets[i]}\")\n","    print(f\"Predicted: {decoded_sentences[i]}\")\n","    print(\"-\" * 50)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
